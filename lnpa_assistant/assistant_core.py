import os
import re
import time
import json
import logging
from datetime import datetime
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
# –£–±–∏—Ä–∞–µ–º –∏–º–ø–æ—Ä—Ç OpenAI, –¥–æ–±–∞–≤–ª—è–µ–º DeepSeek
try:
    from deepseek import DeepSeek
    DEEPSEEK_AVAILABLE = True
except ImportError:
    DEEPSEEK_AVAILABLE = False
import PyPDF2
from docx import Document
import hashlib

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class DocumentChunk:
    text: str
    metadata: Dict
    embedding: Optional[List[float]] = None

class EnhancedLNPAssistant:
    def __init__(self, data_dir="documents", persist_dir="./chroma_db", config_path="config.json"):
        self.data_dir = data_dir
        self.persist_dir = persist_dir
        self.config_path = config_path
        self.config = self.load_config()
        
        self.documents = []
        self.embedding_model = None
        self.llm_client = None
        self.vector_db = None
        self.collection = None
        self.templates = {}
        
        self.setup_system()
    
    def load_config(self) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
        default_config = {
            "embedding_model": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            "llm_provider": os.getenv("LLM_PROVIDER", "deepseek"),  # deepseek, openai, azure, local
            "deepseek_api_key": "",
            "deepseek_model": "deepseek-chat",
            "openai_api_key": "",
            "openai_model": "gpt-4o",
            "azure_endpoint": "",
            "azure_api_key": "",
            "azure_deployment": "",
            "anthropic_api_key": "",
            "anthropic_model": "claude-3-sonnet-20240229",
            "local_model": "qwen2.5-coder-7b-instruct-q4_0.gguf",
            "chunk_size": 1000,
            "chunk_overlap": 200,
            "max_results": 5,
            "temperature": 0.1,
            "max_tokens": 500
        }
        
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                default_config.update(user_config)
        except FileNotFoundError:
            logger.info("–ö–æ–Ω—Ñ–∏–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
        
        # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–º–µ—é—Ç –≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        self.override_config_with_env(default_config)
        
        return default_config
    
    def override_config_with_env(self, config: Dict):
        """–ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
        env_mappings = {
            "LLM_PROVIDER": "llm_provider",
            "DEEPSEEK_API_KEY": "deepseek_api_key",
            "DEEPSEEK_MODEL": "deepseek_model",
            "OPENAI_API_KEY": "openai_api_key",
            "OPENAI_MODEL": "openai_model",
            "AZURE_ENDPOINT": "azure_endpoint",
            "AZURE_API_KEY": "azure_api_key",
            "AZURE_DEPLOYMENT": "azure_deployment"
        }
        
        for env_var, config_key in env_mappings.items():
            env_value = os.getenv(env_var)
            if env_value:
                config[config_key] = env_value
                logger.debug(f"–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è {env_var} –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏–ª–∞ {config_key}")
    
    def setup_system(self):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏—Å—Ç–µ–º—ã"""
        logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –õ–ü–ê/–õ–ù–ü–ê...")
        
        try:
            self.load_models()
            self.setup_vector_db()
            self.load_all_documents()
            self.load_templates()
            logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞!")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            raise
    
    def load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
        
        try:
            # –≠–º–±–µ–¥–¥–∏–Ω–≥ –º–æ–¥–µ–ª—å –≤—Å–µ–≥–¥–∞ –ª–æ–∫–∞–ª—å–Ω–∞—è
            self.embedding_model = SentenceTransformer(self.config["embedding_model"])
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LLM –∫–ª–∏–µ–Ω—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
            llm_provider = self.config["llm_provider"]
            
            if llm_provider == "deepseek":
                if not DEEPSEEK_AVAILABLE:
                    raise ValueError("DeepSeek –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install deepseek-api")
                
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è -> –∫–æ–Ω—Ñ–∏–≥ -> –æ—à–∏–±–∫–∞
                api_key = os.getenv("DEEPSEEK_API_KEY") or self.config["deepseek_api_key"]
                if not api_key:
                    raise ValueError("DeepSeek API key not found. Set DEEPSEEK_API_KEY environment variable or add to config")
                
                self.llm_client = DeepSeek(api_key=api_key)
                self.llm_model = os.getenv("DEEPSEEK_MODEL") or self.config.get("deepseek_model", "deepseek-chat")
                logger.info(f"‚úÖ DeepSeek –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (–º–æ–¥–µ–ª—å: {self.llm_model})")
                
            elif llm_provider == "openai":
                api_key = os.getenv("OPENAI_API_KEY") or self.config["openai_api_key"]
                if not api_key:
                    raise ValueError("OpenAI API key not found. Set OPENAI_API_KEY environment variable or add to config")
                
                from openai import OpenAI
                self.llm_client = OpenAI(api_key=api_key)
                self.llm_model = os.getenv("OPENAI_MODEL") or self.config["openai_model"]
                logger.info(f"‚úÖ OpenAI –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (–º–æ–¥–µ–ª—å: {self.llm_model})")
                
            elif llm_provider == "azure":
                endpoint = os.getenv("AZURE_ENDPOINT") or self.config["azure_endpoint"]
                api_key = os.getenv("AZURE_API_KEY") or self.config["azure_api_key"]
                
                if not endpoint or not api_key:
                    raise ValueError("Azure endpoint or API key not found")
                
                from openai import OpenAI
                self.llm_client = OpenAI(
                    api_key=api_key,
                    base_url=endpoint,
                    api_version="2023-12-01-preview"
                )
                self.llm_model = os.getenv("AZURE_DEPLOYMENT") or self.config["azure_deployment"]
                logger.info(f"‚úÖ Azure OpenAI –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (–¥–µ–ø–ª–æ–π–º–µ–Ω—Ç: {self.llm_model})")
                
            elif llm_provider == "anthropic":
                logger.info("‚ùå Anthropic –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ —ç—Ç–æ–π –≤–µ—Ä—Å–∏–∏")
                raise ValueError("Anthropic support not implemented")
                
            elif llm_provider == "local":
                from gpt4all import GPT4All
                self.llm_client = GPT4All(self.config["local_model"])
                self.llm_model = "local"
                logger.info("‚úÖ –õ–æ–∫–∞–ª—å–Ω–∞—è LLM –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                
            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {llm_provider}")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            raise
    
    # –û—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π...
    def setup_vector_db(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        try:
            self.vector_db = chromadb.PersistentClient(path=self.persist_dir)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é
            try:
                self.collection = self.vector_db.get_collection("lnp_documents")
                logger.info("üìä –ù–∞–π–¥–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            except Exception as e:
                logger.info("üìä –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                self.collection = self.vector_db.create_collection(
                    name="lnp_documents",
                    metadata={"description": "–ë–∞–∑–∞ –õ–ü–ê/–õ–ù–ü–ê –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", "created": datetime.now().isoformat()}
                )
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ë–î: {e}")
            raise
    
    def extract_text_from_pdf(self, filepath: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF"""
        try:
            with open(filepath, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                text = ""
                for page in reader.pages:
                    text += page.extract_text() + "\n"
                return text
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF {filepath}: {e}")
            return ""
    
    def extract_text_from_docx(self, filepath: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX"""
        try:
            doc = Document(filepath)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è DOCX {filepath}: {e}")
            return ""
    
    def get_file_hash(self, filepath: str) -> str:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–∞ —Ñ–∞–π–ª–∞"""
        try:
            with open(filepath, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ö–µ—à–∞ {filepath}: {e}")
            return ""
    
    def document_exists(self, filename: str, file_hash: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –ë–î"""
        try:
            results = self.collection.get(
                where={"filename": {"$eq": filename}},
                limit=1
            )
            
            if results['ids']:
                first_metadata = results['metadatas'][0]
                if 'file_hash' in first_metadata and first_metadata['file_hash'] == file_hash:
                    return True
            return False
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {filename}: {e}")
            return False
    
    def smart_chunking(self, text: str, filename: str) -> List[DocumentChunk]:
        """–£–º–Ω–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ —Å —É—á–µ—Ç–æ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        text = re.sub(r'\s+', ' ', text).strip()
        
        if not text:
            return []
        
        chunks = []
        chunk_size = self.config["chunk_size"]
        overlap = self.config["chunk_overlap"]
        
        for i in range(0, len(text), chunk_size - overlap):
            chunk_text = text[i:i + chunk_size]
            if len(chunk_text) > 100:
                chunks.append(DocumentChunk(
                    text=chunk_text,
                    metadata={
                        'filename': filename,
                        'chunk_id': len(chunks),
                        'start_char': i,
                        'end_char': i + len(chunk_text)
                    }
                ))
        
        return chunks
    
    def load_all_documents(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir)
            logger.info(f"üìÅ –°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {self.data_dir}")
            return
        
        supported_extensions = ['.txt', '.pdf', '.docx']
        files = []
        
        for ext in supported_extensions:
            files.extend([f for f in os.listdir(self.data_dir) if f.lower().endswith(ext)])
        
        logger.info(f"üìö –ù–∞–π–¥–µ–Ω–æ {len(files)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        total_loaded = 0
        for filename in files:
            if self.process_document(filename):
                total_loaded += 1
        
        logger.info(f"üéØ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {total_loaded} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    def process_document(self, filename: str) -> bool:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        filepath = os.path.join(self.data_dir, filename)
        
        if not os.path.exists(filepath):
            logger.warning(f"‚ö†Ô∏è –§–∞–π–ª {filename} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            return False
        
        file_hash = self.get_file_hash(filepath)
        if not file_hash:
            return False
        
        if self.document_exists(filename, file_hash):
            logger.info(f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç {filename} —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω")
            return True
        
        try:
            text = ""
            if filename.lower().endswith('.txt'):
                with open(filepath, 'r', encoding='utf-8') as f:
                    text = f.read()
            elif filename.lower().endswith('.pdf'):
                text = self.extract_text_from_pdf(filepath)
            elif filename.lower().endswith('.docx'):
                text = self.extract_text_from_docx(filepath)
            
            if not text:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ {filename}")
                return False
            
            chunks = self.smart_chunking(text, filename)
            if not chunks:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–±–∏—Ç—å –Ω–∞ —á–∞–Ω–∫–∏ {filename}")
                return False
            
            self.add_chunks_to_db(chunks, filename, file_hash)
            logger.info(f"‚úÖ {filename} - {len(chunks)} —á–∞–Ω–∫–æ–≤")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {filename}: {e}")
            return False
    
    def add_chunks_to_db(self, chunks: List[DocumentChunk], filename: str, file_hash: str):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –≤ –ë–î"""
        texts = [chunk.text for chunk in chunks]
        
        try:
            embeddings = self.embedding_model.encode(texts).tolist()
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {filename}: {e}")
            return
        
        metadatas = []
        ids = []
        
        for i, chunk in enumerate(chunks):
            metadata = chunk.metadata.copy()
            metadata.update({
                'file_hash': file_hash,
                'timestamp': datetime.now().isoformat()
            })
            metadatas.append(metadata)
            ids.append(f"{filename}_{i}_{hashlib.md5(chunk.text.encode()).hexdigest()[:8]}")
        
        try:
            self.collection.add(
                embeddings=embeddings,
                documents=texts,
                metadatas=metadatas,
                ids=ids
            )
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –ë–î: {e}")
    
    def semantic_search(self, query: str, n_results: int = 3) -> List[Tuple[str, Dict]]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            query_embedding = self.embedding_model.encode([query]).tolist()
            
            results = self.collection.query(
                query_embeddings=query_embedding,
                n_results=n_results
            )
            
            formatted_results = []
            for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):
                formatted_results.append((doc, metadata))
            
            return formatted_results
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def generate_with_online_model(self, prompt: str, max_tokens: int = 500, temperature: float = 0.1) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–Ω–ª–∞–π–Ω-–º–æ–¥–µ–ª–µ–π"""
        try:
            llm_provider = self.config["llm_provider"]
            
            if llm_provider == "deepseek":
                response = self.llm_client.chat.completions.create(
                    model=self.llm_model,
                    messages=[
                        {"role": "system", "content": "–¢—ã - AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–º–∏ –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –õ–ü–ê/–õ–ù–ü–ê. –û—Ç–≤–µ—á–∞–π —Ç–æ—á–Ω–æ –∏ –ø–æ –¥–µ–ª—É."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=max_tokens,
                    temperature=temperature,
                    stream=False
                )
                return response.choices[0].message.content
                
            elif llm_provider in ["openai", "azure"]:
                response = self.llm_client.chat.completions.create(
                    model=self.llm_model,
                    messages=[
                        {"role": "system", "content": "–¢—ã - AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–º–∏ –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –õ–ü–ê/–õ–ù–ü–ê. –û—Ç–≤–µ—á–∞–π —Ç–æ—á–Ω–æ –∏ –ø–æ –¥–µ–ª—É."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                return response.choices[0].message.content
                
            elif llm_provider == "local":
                return self.llm_client.generate(prompt, max_tokens=max_tokens, temp=temperature)
                
            else:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {llm_provider}")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}"
    
    def ask_question(self, question: str) -> Dict:
        """–û—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        start_time = time.time()
        
        search_results = self.semantic_search(question, n_results=3)
        
        if not search_results:
            return {
                "answer": "–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.",
                "sources": [],
                "response_time": time.time() - start_time
            }
        
        context_parts = []
        sources = []
        
        for i, (text, metadata) in enumerate(search_results):
            context_parts.append(f"[–î–æ–∫—É–º–µ–Ω—Ç {i+1}: {metadata['filename']}]\n{text}")
            sources.append({
                "filename": metadata['filename'],
                "chunk_id": metadata['chunk_id'],
                "text_excerpt": text[:200] + "..."
            })
        
        context = "\n\n".join(context_parts)
        
        prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å. –ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ —Ü–∏—Ç–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç—ã.

–î–æ–∫—É–º–µ–Ω—Ç—ã:
{context}

–í–æ–ø—Ä–æ—Å: {question}

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
1. –û—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
2. –£–∫–∞–∂–∏ –Ω–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
3. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç - —Å–æ–æ–±—â–∏ –æ–± —ç—Ç–æ–º
4. –ë—É–¥—å –ª–∞–∫–æ–Ω–∏—á–Ω—ã–º

–û—Ç–≤–µ—Ç:"""
        
        try:
            answer = self.generate_with_online_model(prompt, max_tokens=300, temperature=0.1)
            
            return {
                "answer": answer.strip(),
                "sources": sources,
                "response_time": time.time() - start_time
            }
            
        except Exception as e:
            return {
                "answer": f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}",
                "sources": [],
                "response_time": time.time() - start_time
            }
    
    def load_templates(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —à–∞–±–ª–æ–Ω–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        templates_dir = "templates"
        if not os.path.exists(templates_dir):
            os.makedirs(templates_dir)
            self.create_default_templates(templates_dir)
        
        template_files = [f for f in os.listdir(templates_dir) if f.endswith('.json')]
        
        for template_file in template_files:
            filepath = os.path.join(templates_dir, template_file)
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    template_name = template_file.replace('.json', '')
                    self.templates[template_name] = json.load(f)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —à–∞–±–ª–æ–Ω–∞ {template_file}: {e}")
        
        logger.info(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.templates)} —à–∞–±–ª–æ–Ω–æ–≤")
    
    def create_default_templates(self, templates_dir):
        """–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö —à–∞–±–ª–æ–Ω–æ–≤"""
        tz_template = {
            "name": "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ",
            "sections": [
                {"name": "–û–±—â–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è", "fields": ["–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–æ—Å–Ω–æ–≤–∞–Ω–∏–µ", "—Ü–µ–ª–∏"]},
                {"name": "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è", "fields": ["—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è", "—Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã", "—Å—Ä–æ–∫–∏"]},
                {"name": "–ü–æ—Ä—è–¥–æ–∫ –ø—Ä–∏–µ–º–∫–∏", "fields": ["–ø—Ä–æ—Ü–µ–¥—É—Ä–∞", "–∫—Ä–∏—Ç–µ—Ä–∏–∏", "–¥–æ–∫—É–º–µ–Ω—Ç—ã"]}
            ],
            "prompt": "–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π: {context}"
        }
        
        try:
            with open(os.path.join(templates_dir, "–¢–ó.json"), 'w', encoding='utf-8') as f:
                json.dump(tz_template, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —à–∞–±–ª–æ–Ω–∞ –¢–ó: {e}")
    
    def generate_document(self, doc_type: str, requirements: str) -> Dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ —à–∞–±–ª–æ–Ω—É"""
        if doc_type not in self.templates:
            return {"error": f"–®–∞–±–ª–æ–Ω {doc_type} –Ω–µ –Ω–∞–π–¥–µ–Ω"}
        
        template = self.templates[doc_type]
        
        search_results = self.semantic_search(requirements, n_results=2)
        context = " ".join([text for text, _ in search_results])
        
        prompt = template["prompt"].format(context=context, requirements=requirements)
        
        try:
            generated_content = self.generate_with_online_model(prompt, max_tokens=500, temperature=0.2)
            
            return {
                "document_type": doc_type,
                "content": generated_content.strip(),
                "sections": template["sections"],
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            return {"error": f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}"}
    
    def interactive_mode(self):
        """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã"""
        print("\n" + "="*60)
        print(f"ü§ñ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –õ–ü–ê/–õ–ù–ü–ê –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
        print(f"üì° –†–µ–∂–∏–º: {self.config['llm_provider'].upper()}")
        print("–ö–æ–º–∞–Ω–¥—ã:")
        print("  /–≤–æ–ø—Ä–æ—Å [—Ç–µ–∫—Å—Ç] - –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å")
        print("  /–≥–µ–Ω–µ—Ä–∞—Ü–∏—è [—Ç–∏–ø] [—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è] - —Å–æ–∑–¥–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç")
        print("  /—à–∞–±–ª–æ–Ω—ã - —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —à–∞–±–ª–æ–Ω–æ–≤")
        print("  /—Å—Ç–∞—Ç—É—Å - –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ")
        print("  /–≤—ã—Ö–æ–¥ - –∑–∞–≤–µ—Ä—à–∏—Ç—å —Ä–∞–±–æ—Ç—É")
        print("="*60)
        
        while True:
            try:
                user_input = input("\nüéØ –í–≤–µ–¥–∏—Ç–µ –∫–æ–º–∞–Ω–¥—É: ").strip()
                
                if user_input.lower() in ['/–≤—ã—Ö–æ–¥', '/exit']:
                    break
                
                elif user_input.startswith('/–≤–æ–ø—Ä–æ—Å '):
                    question = user_input[8:].strip()
                    if question:
                        result = self.ask_question(question)
                        print(f"\nü§ñ –û—Ç–≤–µ—Ç ({result['response_time']:.1f}—Å–µ–∫):")
                        print(result['answer'])
                        if result['sources']:
                            print("\nüìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏:")
                            for source in result['sources']:
                                print(f"   - {source['filename']} (—Ñ—Ä–∞–≥–º–µ–Ω—Ç {source['chunk_id']})")
                
                elif user_input.startswith('/–≥–µ–Ω–µ—Ä–∞—Ü–∏—è '):
                    parts = user_input[11:].split(' ', 1)
                    if len(parts) == 2:
                        doc_type, requirements = parts
                        result = self.generate_document(doc_type, requirements)
                        if 'error' not in result:
                            print(f"\nüìÑ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –¥–æ–∫—É–º–µ–Ω—Ç: {result['document_type']}")
                            print(result['content'])
                        else:
                            print(f"‚ùå {result['error']}")
                    else:
                        print("‚ùå –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: /–≥–µ–Ω–µ—Ä–∞—Ü–∏—è [—Ç–∏–ø] [—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è]")
                
                elif user_input == '/—à–∞–±–ª–æ–Ω—ã':
                    print("\nüìã –î–æ—Å—Ç—É–ø–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã:")
                    for template in self.templates.keys():
                        print(f"   - {template}")
                
                elif user_input == '/—Å—Ç–∞—Ç—É—Å':
                    try:
                        count = self.collection.count()
                        print(f"\nüìä –°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã:")
                        print(f"   –ü—Ä–æ–≤–∞–π–¥–µ—Ä: {self.config['llm_provider']}")
                        print(f"   –ú–æ–¥–µ–ª—å: {self.llm_model}")
                        print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ: {count}")
                        print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ —à–∞–±–ª–æ–Ω–æ–≤: {len(self.templates)}")
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞: {e}")
                
                else:
                    print("‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞")
                    
            except KeyboardInterrupt:
                print("\nüëã –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
                break
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    try:
        assistant = EnhancedLNPAssistant()
        assistant.interactive_mode()
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())