import os
import json
import pickle
import numpy as np
import re
from datetime import datetime
from typing import List, Dict, Tuple, Optional
import warnings
warnings.filterwarnings("ignore")

# –ë–∞–∑–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.feature_extraction.text import TfidfVectorizer
except ImportError as e:
    print(f"‚ùå –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏: {e}")
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sentence-transformers scikit-learn")
    exit(1)

# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å DOCX
try:
    from docx import Document
    from docx.shared import Inches
    from docx.enum.text import WD_ALIGN_PARAGRAPH
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False
    print("‚ö†Ô∏è python-docx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è DOCX –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞
try:
    import torch
    from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("‚ö†Ô∏è Transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")

class DocumentProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    @staticmethod
    def extract_qa_pairs(text: str) -> List[Dict[str, str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        qa_pairs = []
        
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –±–ª–æ–∫–∏ –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º
        question_blocks = re.split(r'\n(?=–í–æ–ø—Ä–æ—Å:)', text.strip())
        
        for block in question_blocks:
            block = block.strip()
            if not block or '–í–æ–ø—Ä–æ—Å:' not in block:
                continue
                
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–æ–ø—Ä–æ—Å
            question_match = re.search(r'–í–æ–ø—Ä–æ—Å:\s*(.*?)(?=\n–û—Ç–≤–µ—Ç:|$)', block, re.DOTALL)
            if not question_match:
                continue
            question = question_match.group(1).strip()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç
            answer_match = re.search(r'–û—Ç–≤–µ—Ç:\s*(.*?)(?=\n–î–æ–∫—É–º–µ–Ω—Ç:|$)', block, re.DOTALL)
            if not answer_match:
                continue
            answer = answer_match.group(1).strip()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            document_match = re.search(r'–î–æ–∫—É–º–µ–Ω—Ç:\s*(.*?)(?=\n–°—Å—ã–ª–∫–∞:|$)', block, re.DOTALL)
            document = document_match.group(1).strip() if document_match else ""
            
            link_match = re.search(r'–°—Å—ã–ª–∫–∞:\s*(.*?)$', block, re.MULTILINE)
            link = link_match.group(1).strip() if link_match else ""
            
            if question and answer:
                qa_pairs.append({
                    'question': question,
                    'answer': answer,
                    'document': document,
                    'link': link,
                    'full_text': block
                })
        
        return qa_pairs
    
    @staticmethod
    def clean_text(text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n+', '\n', text)
        return text.strip()
    
    @staticmethod
    def extract_document_metadata(text: str) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        metadata = {}
        
        # –ü–æ–∏—Å–∫ –Ω–æ–º–µ—Ä–∞ –∑–∞–∫–æ–Ω–∞
        law_number = re.search(r'‚Ññ\s*(\d+-\w+)', text)
        if law_number:
            metadata['law_number'] = law_number.group(1)
        
        # –ü–æ–∏—Å–∫ –¥–∞—Ç—ã
        date_match = re.search(r'–æ—Ç\s*(\d{1,2}\s+\w+\s+\d{4})', text)
        if date_match:
            metadata['date'] = date_match.group(1)
        
        # –ü–æ–∏—Å–∫ –æ—Ä–≥–∞–Ω–∞
        if '–†–µ—Å–ø—É–±–ª–∏–∫–∏ –ë–µ–ª–∞—Ä—É—Å—å' in text:
            metadata['authority'] = '–†–µ—Å–ø—É–±–ª–∏–∫–∞ –ë–µ–ª–∞—Ä—É—Å—å'
        
        return metadata

class KnowledgeBase:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.embedder = SentenceTransformer(model_name)
        self.documents: List[Dict] = []
        self.embeddings: Optional[np.ndarray] = None
        self.tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words=None)
        self.tfidf_matrix = None
        self.index_file = "knowledge_base_index.pkl"
        
        print(f"‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å –º–æ–¥–µ–ª—å—é: {model_name}")
    
    def add_document_from_file(self, file_path: str) -> int:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except UnicodeDecodeError:
            with open(file_path, 'r', encoding='windows-1251') as f:
                content = f.read()
        
        qa_pairs = DocumentProcessor.extract_qa_pairs(content)
        metadata = DocumentProcessor.extract_document_metadata(content)
        
        added_count = 0
        for qa in qa_pairs:
            qa.update(metadata)
            qa['source_file'] = os.path.basename(file_path)
            qa['added_date'] = datetime.now().isoformat()
            self.documents.append(qa)
            added_count += 1
        
        print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω —Ñ–∞–π–ª {file_path}: –¥–æ–±–∞–≤–ª–µ–Ω–æ {added_count} –∑–∞–ø–∏—Å–µ–π")
        return added_count
    
    def load_documents_from_folder(self, folder_path: str) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –ø–∞–ø–∫–∏"""
        if not os.path.exists(folder_path):
            raise FileNotFoundError(f"–ü–∞–ø–∫–∞ {folder_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        
        txt_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]
        if not txt_files:
            raise ValueError(f"–í –ø–∞–ø–∫–µ {folder_path} –Ω–µ—Ç .txt —Ñ–∞–π–ª–æ–≤")
        
        total_added = 0
        for filename in txt_files:
            file_path = os.path.join(folder_path, filename)
            added = self.add_document_from_file(file_path)
            total_added += added
        
        print(f"üìö –í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {total_added} –∑–∞–ø–∏—Å–µ–π –∏–∑ {len(txt_files)} —Ñ–∞–π–ª–æ–≤")
        self.build_index()
    
    def build_index(self) -> None:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        if not self.documents:
            raise ValueError("–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
        
        print("üî® –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞...")
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        texts = []
        for doc in self.documents:
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –≤–æ–ø—Ä–æ—Å –∏ –æ—Ç–≤–µ—Ç –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
            combined_text = f"{doc['question']} {doc['answer']}"
            texts.append(combined_text)
        
        # –°—Ç—Ä–æ–∏–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
        self.embeddings = self.embedder.encode(texts, convert_to_numpy=True)
        
        # –°—Ç—Ä–æ–∏–º TF-IDF –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)
        
        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω –¥–ª—è {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    def save_index(self, filepath: str = None) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞"""
        filepath = filepath or self.index_file
        data = {
            'documents': self.documents,
            'embeddings': self.embeddings,
            'tfidf_vectorizer': self.tfidf_vectorizer,
            'tfidf_matrix': self.tfidf_matrix
        }
        with open(filepath, 'wb') as f:
            pickle.dump(data, f)
        print(f"üíæ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {filepath}")
    
    def load_index(self, filepath: str = None) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞"""
        filepath = filepath or self.index_file
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"–§–∞–π–ª –∏–Ω–¥–µ–∫—Å–∞ {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
        
        self.documents = data['documents']
        self.embeddings = data['embeddings']
        self.tfidf_vectorizer = data['tfidf_vectorizer']
        self.tfidf_matrix = data['tfidf_matrix']
        
        print(f"üì• –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {filepath}: {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    def search(self, query: str, top_k: int = 5, method: str = "combined") -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        if not self.documents or self.embeddings is None:
            raise ValueError("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞ –∏–ª–∏ –Ω–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∞")
        
        query_cleaned = DocumentProcessor.clean_text(query)
        
        if method == "semantic":
            return self._semantic_search(query_cleaned, top_k)
        elif method == "tfidf":
            return self._tfidf_search(query_cleaned, top_k)
        else:  # combined
            return self._combined_search(query_cleaned, top_k)
    
    def _semantic_search(self, query: str, top_k: int) -> List[Dict]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
        query_embedding = self.embedder.encode([query], convert_to_numpy=True)
        similarities = cosine_similarity(query_embedding, self.embeddings)[0]
        
        top_indices = similarities.argsort()[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            result = self.documents[idx].copy()
            result['similarity_score'] = float(similarities[idx])
            result['search_method'] = 'semantic'
            results.append(result)
        
        return results
    
    def _tfidf_search(self, query: str, top_k: int) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ TF-IDF"""
        query_tfidf = self.tfidf_vectorizer.transform([query])
        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)[0]
        
        top_indices = similarities.argsort()[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0:  # –¢–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                result = self.documents[idx].copy()
                result['similarity_score'] = float(similarities[idx])
                result['search_method'] = 'tfidf'
                results.append(result)
        
        return results
    
    def _combined_search(self, query: str, top_k: int) -> List[Dict]:
        """–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫"""
        semantic_results = self._semantic_search(query, top_k)
        tfidf_results = self._tfidf_search(query, top_k)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –≤–µ—Å–∞–º–∏
        combined_scores = {}
        
        for result in semantic_results:
            doc_id = id(result['full_text'])
            combined_scores[doc_id] = {
                'document': result,
                'semantic_score': result['similarity_score'],
                'tfidf_score': 0.0
            }
        
        for result in tfidf_results:
            doc_id = id(result['full_text'])
            if doc_id in combined_scores:
                combined_scores[doc_id]['tfidf_score'] = result['similarity_score']
            else:
                combined_scores[doc_id] = {
                    'document': result,
                    'semantic_score': 0.0,
                    'tfidf_score': result['similarity_score']
                }
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä
        for doc_id in combined_scores:
            semantic_weight = 0.7
            tfidf_weight = 0.3
            combined_scores[doc_id]['combined_score'] = (
                semantic_weight * combined_scores[doc_id]['semantic_score'] +
                tfidf_weight * combined_scores[doc_id]['tfidf_score']
            )
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —Å–∫–æ—Ä—É
        sorted_results = sorted(combined_scores.values(), 
                              key=lambda x: x['combined_score'], 
                              reverse=True)
        
        final_results = []
        for item in sorted_results[:top_k]:
            result = item['document'].copy()
            result['similarity_score'] = item['combined_score']
            result['search_method'] = 'combined'
            final_results.append(result)
        
        return final_results

class AdvancedDocumentGenerator:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ DOCX"""
    
    def __init__(self):
        self.document = None
        if not DOCX_AVAILABLE:
            print("‚ö†Ô∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è DOCX –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ python-docx")
    
    def create_document(self, doc_type: str, data: Dict[str, str]) -> 'Document':
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ —Ç–∏–ø—É"""
        if not DOCX_AVAILABLE:
            raise ImportError("python-docx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        self.document = Document()
        self._set_default_font()

        if doc_type == "–¢–ó":
            return self._create_technical_specification(data)
        elif doc_type == "–ó–ù–ó":
            return self._create_procurement_order(data)
        elif doc_type == "–ö–ü":
            return self._create_competitive_proposal(data)
        else:
            raise ValueError("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞")

    def _set_default_font(self):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ —à—Ä–∏—Ñ—Ç–∞"""
        style = self.document.styles['Normal']
        font = style.font
        font.name = 'Times New Roman'
        font.size = Inches(0.14)  # 12pt

    def _add_header(self, data):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à–∞–ø–∫–∏ '–£—Ç–≤–µ—Ä–∂–¥–∞—é'"""
        self.document.add_paragraph()

        p = self.document.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.RIGHT
        p.add_run("–£—Ç–≤–µ—Ä–∂–¥–∞—é\n").bold = True

        p = self.document.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.RIGHT
        p.add_run(f"{data.get('approver_position', '–ì–ª–∞–≤–Ω—ã–π –≤—Ä–∞—á')}\n")
        p.add_run(f"{data.get('organization', '–£—á—Ä–µ–∂–¥–µ–Ω–∏–µ')}\n")

        p = self.document.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.RIGHT
        p.add_run("___________________ ")
        p.add_run(f"{data.get('approver_name', '–ò.–ò. –ò–≤–∞–Ω–æ–≤')}\n")

        p = self.document.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.RIGHT
        p.add_run(f'¬´____¬ª ___________ {datetime.now().year} –≥.')

        self.document.add_paragraph()

    def _add_title(self, title):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        p = self.document.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p.add_run(title.upper()).bold = True
        self.document.add_paragraph()

    def _add_footer(self, data):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–¥–ø–∏—Å–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞"""
        for _ in range(3):
            self.document.add_paragraph()

        p = self.document.add_paragraph()
        p.add_run("–†–∞–∑—Ä–∞–±–æ—Ç–∞–ª:\n")

        p = self.document.add_paragraph()
        p.add_run(f"{data.get('developer_position', '–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç')}\n")

        p = self.document.add_paragraph()
        p.add_run("___________________ ")
        p.add_run(f"{data.get('developer_name', '–ü.–°. –ü–µ—Ç—Ä–æ–≤')}")

        p = self.document.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.LEFT
        p.add_run(f"{data.get('location', '–ú–∏–Ω—Å–∫')} {datetime.now().year}")

    def _create_technical_specification(self, data):
        """–°–æ–∑–¥–∞–Ω–∏–µ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è"""
        self._add_header(data)
        doc_title = data.get('title', '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ')
        self._add_title(doc_title)

        p = self.document.add_paragraph()
        p.add_run("1. –û–±—â–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è\n").bold = True

        requirements = data.get('general_requirements', [])
        for i, req in enumerate(requirements, 1):
            p = self.document.add_paragraph()
            p.add_run(f"1.{i} {req}")

        p = self.document.add_paragraph()
        p.add_run("2. –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ç–æ–≤–∞—Ä—É:\n").bold = True

        table_data = data.get('technical_requirements', [])
        if table_data:
            table = self.document.add_table(rows=1, cols=3)
            table.style = 'Table Grid'

            hdr_cells = table.rows[0].cells
            hdr_cells[0].text = '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ'
            hdr_cells[1].text = '–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏'
            hdr_cells[2].text = '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'

            for item in table_data:
                row_cells = table.add_row().cells
                row_cells[0].text = item.get('name', '')
                row_cells[1].text = item.get('characteristics', '')
                row_cells[2].text = item.get('quantity', '')

        p = self.document.add_paragraph()
        p.add_run("3. –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∫–∞—á–µ—Å—Ç–≤—É –ø–æ—Å—Ç–∞–≤–ª—è–µ–º–æ–≥–æ —Ç–æ–≤–∞—Ä–∞\n").bold = True

        quality_requirements = data.get('quality_requirements', [])
        for i, req in enumerate(quality_requirements, 1):
            p = self.document.add_paragraph()
            p.add_run(f"3.{i} {req}")

        p = self.document.add_paragraph()
        p.add_run("4. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è\n").bold = True

        additional_requirements = data.get('additional_requirements', [])
        for i, req in enumerate(additional_requirements, 1):
            p = self.document.add_paragraph()
            p.add_run(f"4.{i} {req}")

        p = self.document.add_paragraph()
        p.add_run("5. –≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ\n").bold = True

        economic_data = data.get('economic_justification', {})
        p = self.document.add_paragraph()
        p.add_run(f"5.1 –ò—Å—Ç–æ—á–Ω–∏–∫ —Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏—è: {economic_data.get('funding_source', '–±—é–¥–∂–µ—Ç')}")

        p = self.document.add_paragraph()
        p.add_run(f"5.2 –í–∏–¥ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –∑–∞–∫—É–ø–∫–∏: {economic_data.get('procurement_type', '–∑–∞–∫—É–ø–∫–∞ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞')}")

        p = self.document.add_paragraph()
        p.add_run(f"5.3 –£—Å–ª–æ–≤–∏—è –æ–ø–ª–∞—Ç—ã: {economic_data.get('payment_terms', '–ø–æ —Ñ–∞–∫—Ç—É –ø–æ—Å—Ç–∞–≤–∫–∏')}")

        self._add_footer(data)
        return self.document

    def _create_procurement_order(self, data):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ó–∞–¥–∞–Ω–∏—è –Ω–∞ –∑–∞–∫—É–ø–∫—É"""
        self._add_header(data)
        self._add_title(data.get('title', '–ó–∞–¥–∞–Ω–∏–µ –Ω–∞ –∑–∞–∫—É–ø–∫—É'))

        sections = [
            ("1. –û–±—â–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è", data.get('general_requirements', [])),
            ("2. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", data.get('technical_specifications', [])),
            ("3. –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∫–∞—á–µ—Å—Ç–≤—É", data.get('quality_requirements', [])),
            ("4. –£—Å–ª–æ–≤–∏—è –∏ —Å—Ä–æ–∫–∏ –ø–æ—Å—Ç–∞–≤–∫–∏", data.get('delivery_terms', [])),
            ("5. –ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏ –∑–∞—è–≤–æ–∫", data.get('evaluation_criteria', [])),
            ("6. –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —É—á–∞—Å—Ç–Ω–∏–∫–∞–º", data.get('participant_requirements', [])),
        ]

        for section_title, section_content in sections:
            p = self.document.add_paragraph()
            p.add_run(f"{section_title}\n").bold = True

            for i, item in enumerate(section_content, 1):
                p = self.document.add_paragraph()
                p.add_run(f"{section_title.split('.')[0]}.{i} {item}")

        p = self.document.add_paragraph()
        p.add_run("7. –§–æ—Ä–º–∞ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n").bold = True

        table = self.document.add_table(rows=2, cols=8)
        table.style = 'Table Grid'

        headers = ['‚Ññ –ø/–ø', '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–º–µ—Ç–∞ –∑–∞–∫—É–ø–∫–∏', '–ö–æ–ª-–≤–æ', '–ï–¥. –∏–∑–º–µ—Ä–µ–Ω–∏—è',
                   '–°—Ç—Ä–∞–Ω–∞ –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏—è —Ç–æ–≤–∞—Ä–∞', '–¶–µ–Ω–∞, –±–µ–ª.—Ä—É–±.', '–°—Ç–æ-—Å—Ç—å, –±–µ–ª.—Ä—É–±.', '–°—Ç–æ-—Å—Ç—å, –±–µ–ª.—Ä—É–±. —Å –ù–î–°']

        hdr_cells = table.rows[0].cells
        for i, header in enumerate(headers):
            hdr_cells[i].text = header

        example_cells = table.rows[1].cells
        for i in range(len(headers)):
            example_cells[i].text = '—Ö'

        self._add_footer(data)
        return self.document

    def _create_competitive_proposal(self, data):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ö–æ–Ω–∫—É—Ä—Å–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        self._add_header(data)
        self._add_title(data.get('title', '–ö–æ–Ω–∫—É—Ä—Å–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ'))

        p = self.document.add_paragraph()
        p.add_run("–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è\n").bold = True
        p = self.document.add_paragraph()
        p.add_run(data.get('annotation', '–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ–±—ä–µ–º–æ–º –Ω–µ –±–æ–ª–µ–µ —á–µ—Ç—ã—Ä–µ—Ö –ª–∏—Å—Ç–æ–≤.'))

        sections = [
            ("–ö–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ", data.get('commercial_proposal', [])),
            ("–¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ", data.get('technical_proposal', [])),
            ("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–º–ø–∞–Ω–∏–∏", data.get('company_info', [])),
            ("–ì–∞—Ä–∞–Ω—Ç–∏–π–Ω—ã–µ –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞", data.get('warranty_obligations', [])),
        ]

        for section_title, section_content in sections:
            p = self.document.add_paragraph()
            p.add_run(f"{section_title}\n").bold = True

            for i, item in enumerate(section_content, 1):
                p = self.document.add_paragraph()
                p.add_run(f"{i}. {item}")

        self._add_footer(data)
        return self.document

    def save_document(self, filename: str) -> str:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        if self.document:
            self.document.save(filename)
            return f"–î–æ–∫—É–º–µ–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: {filename}"
        else:
            return "–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ —Å–æ–∑–¥–∞–Ω"

class DocumentGenerator:
    """–ö–ª–∞—Å—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã)"""
    
    def __init__(self, knowledge_base: KnowledgeBase):
        self.kb = knowledge_base
        self.templates = self._load_templates()
        self.advanced_generator = AdvancedDocumentGenerator() if DOCX_AVAILABLE else None
    
    def _load_templates(self) -> Dict[str, str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —à–∞–±–ª–æ–Ω–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        return {
            'tz': '''–¢–ï–•–ù–ò–ß–ï–°–ö–û–ï –ó–ê–î–ê–ù–ò–ï
–Ω–∞ {subject}

1. –û–ë–©–ò–ï –ü–û–õ–û–ñ–ï–ù–ò–Ø
{general_provisions}

2. –¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –í–´–ü–û–õ–ù–ï–ù–ò–Æ –†–ê–ë–û–¢
{requirements}

3. –°–†–û–ö–ò –í–´–ü–û–õ–ù–ï–ù–ò–Ø
{timeline}

4. –û–¢–í–ï–¢–°–¢–í–ï–ù–ù–û–°–¢–¨ –°–¢–û–†–û–ù
{responsibility}
''',
            'znz': '''–ó–ê–î–ê–ù–ò–ï –ù–ê –ó–ê–ö–£–ü–ö–£

–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∑–∞–∫—É–ø–∫–∏: {procurement_name}
–ó–∞–∫–∞–∑—á–∏–∫: {customer}
–ü—Ä–µ–¥–º–µ—Ç –∑–∞–∫—É–ø–∫–∏: {subject}

–¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
{technical_requirements}

–ö–û–ú–ú–ï–†–ß–ï–°–ö–ò–ï –£–°–õ–û–í–ò–Ø:
{commercial_conditions}
''',
            'kp': '''–ö–û–ù–ö–£–†–°–ù–û–ï –ü–†–ï–î–õ–û–ñ–ï–ù–ò–ï

–£—á–∞—Å—Ç–Ω–∏–∫: {participant}
–ü—Ä–µ–¥–º–µ—Ç –∫–æ–Ω–∫—É—Ä—Å–∞: {subject}

–¢–ï–•–ù–ò–ß–ï–°–ö–û–ï –ü–†–ï–î–õ–û–ñ–ï–ù–ò–ï:
{technical_proposal}

–ö–û–ú–ú–ï–†–ß–ï–°–ö–û–ï –ü–†–ï–î–õ–û–ñ–ï–ù–ò–ï:
{commercial_proposal}
'''
        }
    
    def generate_document(self, doc_type: str, context: Dict[str, str]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ —à–∞–±–ª–æ–Ω—É"""
        if doc_type not in self.templates:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞: {doc_type}")
        
        template = self.templates[doc_type]
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º —à–∞–±–ª–æ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        try:
            return template.format(**context)
        except KeyError as e:
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä: {e}")
    
    def generate_advanced_document(self, doc_type: str, data: Dict[str, str]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ DOCX"""
        if not DOCX_AVAILABLE:
            return "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è DOCX –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ python-docx"
        
        try:
            doc = self.advanced_generator.create_document(doc_type, data)
            filename = f"{doc_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx"
            return self.advanced_generator.save_document(filename)
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}"

class AIAssistant:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
    
    def __init__(self):
        self.kb = KnowledgeBase()
        self.generator = DocumentGenerator(self.kb)
        self.llm_pipeline = None
        
        if TRANSFORMERS_AVAILABLE:
            self._init_llm()
    
    def _init_llm(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            model_name = "sberbank-ai/rugpt3small_based_on_gpt2"
            self.llm_pipeline = pipeline(
                "text-generation", 
                model=model_name,
                tokenizer=model_name,
                device=0 if torch.cuda.is_available() else -1
            )
            print("ü§ñ –Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å: {e}")
    
    def setup_knowledge_base(self, documents_folder: str) -> None:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        try:
            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å
            self.kb.load_index()
            print("üì• –ó–∞–≥—Ä—É–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å")
        except FileNotFoundError:
            # –°—Ç—Ä–æ–∏–º –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
            print("üî® –°—Ç—Ä–æ–∏–º –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å...")
            self.kb.load_documents_from_folder(documents_folder)
            self.kb.save_index()
    
    def ask(self, question: str, use_llm: bool = False) -> Dict[str, any]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤"""
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        results = self.kb.search(question, top_k=3)
        
        if not results:
            return {
                'answer': "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –≤–∞—à–µ–º—É –≤–æ–ø—Ä–æ—Å—É.",
                'sources': [],
                'confidence': 0.0
            }
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        best_match = results[0]
        
        if use_llm and self.llm_pipeline:
            answer = self._generate_llm_answer(question, results)
        else:
            answer = best_match['answer']
        
        return {
            'answer': answer,
            'sources': [
                {
                    'document': r['document'],
                    'link': r['link'],
                    'confidence': r['similarity_score']
                } for r in results
            ],
            'confidence': best_match['similarity_score']
        }
    
    def _generate_llm_answer(self, question: str, context_results: List[Dict]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM"""
        context = "\n".join([
            f"–í–æ–ø—Ä–æ—Å: {r['question']}\n–û—Ç–≤–µ—Ç: {r['answer']}"
            for r in context_results[:2]
        ])
        
        prompt = f"""–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–µ –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ë–µ–ª–∞—Ä—É—Å—å. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –¥–∞–≤–∞—Ç—å —Ç–æ—á–Ω—ã–µ, –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∞–≤–æ–≤—ã—Ö –∞–∫—Ç–æ–≤, –¥–µ–π—Å—Ç–≤—É—é—â–∏—Ö –Ω–∞ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ –†–ë. –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –°—Å—ã–ª–∞–π—Å—è –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∏–∑–±–µ–≥–∞–π –¥–æ–º—ã—Å–ª–æ–≤, —Å–æ–±–ª—é–¥–∞–π —é—Ä–∏–¥–∏—á–µ—Å–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {question}
–û—Ç–≤–µ—Ç:"""
        
        try:
            response = self.llm_pipeline(
                prompt,
                max_length=len(prompt.split()) + 100,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True
            )
            
            generated_text = response[0]["generated_text"]
            answer = generated_text.split("–û—Ç–≤–µ—Ç:")[-1].strip()
            return answer
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM: {e}")
            return context_results[0]['answer']
    
    def get_statistics(self) -> Dict[str, any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        return {
            'total_documents': len(self.kb.documents),
            'unique_sources': len(set(doc['source_file'] for doc in self.kb.documents)),
            'has_embeddings': self.kb.embeddings is not None,
            'has_llm': self.llm_pipeline is not None,
            'has_docx': DOCX_AVAILABLE
        }

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –≤–≤–æ–¥–∞ –¥–∞–Ω–Ω—ã—Ö
def input_with_default(prompt, default=""):
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–≤–æ–¥–∞ —Å –ø–æ–¥—Å–∫–∞–∑–∫–æ–π –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
    if default:
        user_input = input(f"{prompt} [{default}]: ").strip()
    else:
        user_input = input(f"{prompt}: ").strip()

    return user_input if user_input else default

def input_list(prompt, item_name="–ø—É–Ω–∫—Ç"):
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–≤–æ–¥–∞ —Å–ø–∏—Å–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
    print(prompt)
    print("(–≤–≤–µ–¥–∏—Ç–µ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏, –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è)")

    items = []
    i = 1
    while True:
        item = input(f"{item_name} {i}: ").strip()
        if not item:
            break
        items.append(item)
        i += 1

    return items

def input_technical_requirements():
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–≤–æ–¥–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –≤ –≤–∏–¥–µ —Ç–∞–±–ª–∏—Ü—ã"""
    print("\n–í–≤–æ–¥ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π:")
    print("(–≤–≤–µ–¥–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–≤–∞—Ä–∞, –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è)")

    requirements = []
    i = 1
    while True:
        print(f"\n–¢–æ–≤–∞—Ä {i}:")
        name = input("–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: ").strip()
        if not name:
            break

        characteristics = input("–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏: ").strip()
        quantity = input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: ").strip()

        requirements.append({
            "name": name,
            "characteristics": characteristics,
            "quantity": quantity
        })
        i += 1

    return requirements

def input_economic_justification():
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–≤–æ–¥–∞ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–≥–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è"""
    print("\n–í–≤–æ–¥ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–≥–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è:")

    funding_source = input_with_default("–ò—Å—Ç–æ—á–Ω–∏–∫ —Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏—è", "–æ–±–ª–∞—Å—Ç–Ω–æ–π –±—é–¥–∂–µ—Ç")
    procurement_type = input_with_default("–í–∏–¥ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –∑–∞–∫—É–ø–∫–∏", "–∑–∞–∫—É–ø–∫–∞ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞")
    payment_terms = input_with_default("–£—Å–ª–æ–≤–∏—è –æ–ø–ª–∞—Ç—ã", "–ø–æ —Ñ–∞–∫—Ç—É –ø–æ—Å—Ç–∞–≤–∫–∏ —Ç–æ–≤–∞—Ä–∞, –≤ —Ç–µ—á–µ–Ω–∏–µ 5 –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –¥–Ω–µ–π")

    return {
        "funding_source": funding_source,
        "procurement_type": procurement_type,
        "payment_terms": payment_terms
    }

def get_document_data(doc_type):
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    print(f"\n=== –í–≤–æ–¥ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ '{doc_type}' ===\n")

    data = {}

    data['approver_position'] = input_with_default("–î–æ–ª–∂–Ω–æ—Å—Ç—å —É—Ç–≤–µ—Ä–∂–¥–∞—é—â–µ–≥–æ", "–ì–ª–∞–≤–Ω—ã–π –≤—Ä–∞—á")
    data['organization'] = input_with_default("–ù–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏", "–£—á—Ä–µ–∂–¥–µ–Ω–∏–µ")
    data['approver_name'] = input_with_default("–§–ò–û —É—Ç–≤–µ—Ä–∂–¥–∞—é—â–µ–≥–æ", "–ò.–ò. –ò–≤–∞–Ω–æ–≤")
    data['developer_position'] = input_with_default("–î–æ–ª–∂–Ω–æ—Å—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞", "–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç")
    data['developer_name'] = input_with_default("–§–ò–û —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞", "–ü.–°. –ü–µ—Ç—Ä–æ–≤")
    data['location'] = input_with_default("–ú–µ—Å—Ç–æ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è", "–ú–∏–Ω—Å–∫")
    data['title'] = input_with_default("–ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞", f"{get_doc_full_name(doc_type)}")

    if doc_type == "–¢–ó":
        data['general_requirements'] = input_list("\n–í–≤–µ–¥–∏—Ç–µ –æ–±—â–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:", "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ")
        data['technical_requirements'] = input_technical_requirements()
        data['quality_requirements'] = input_list("\n–í–≤–µ–¥–∏—Ç–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∫–∞—á–µ—Å—Ç–≤—É:", "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ")
        data['additional_requirements'] = input_list("\n–í–≤–µ–¥–∏—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:", "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ")
        data['economic_justification'] = input_economic_justification()

    elif doc_type == "–ó–ù–ó":
        data['general_requirements'] = input_list("\n–í–≤–µ–¥–∏—Ç–µ –æ–±—â–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:", "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ")
        data['technical_specifications'] = input_list("\n–í–≤–µ–¥–∏—Ç–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:", "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞")
        data['quality_requirements'] = input_list("\n–í–≤–µ–¥–∏—Ç–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∫–∞—á–µ—Å—Ç–≤—É:", "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ")
        data['delivery_terms'] = input_list("\n–í–≤–µ–¥–∏—Ç–µ —É—Å–ª–æ–≤–∏—è –∏ —Å—Ä–æ–∫–∏ –ø–æ—Å—Ç–∞–≤–∫–∏:", "—É—Å–ª–æ–≤–∏–µ")
        data['evaluation_criteria'] = input_list("\n–í–≤–µ–¥–∏—Ç–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏ –∑–∞—è–≤–æ–∫:", "–∫—Ä–∏—Ç–µ—Ä–∏–π")
        data['participant_requirements'] = input_list("\n–í–≤–µ–¥–∏—Ç–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —É—á–∞—Å—Ç–Ω–∏–∫–∞–º:", "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ")

    elif doc_type == "–ö–ü":
        data['annotation'] = input_with_default("–í–≤–µ–¥–∏—Ç–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é", "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ–±—ä–µ–º–æ–º –Ω–µ –±–æ–ª–µ–µ —á–µ—Ç—ã—Ä–µ—Ö –ª–∏—Å—Ç–æ–≤.")
        data['commercial_proposal'] = input_list("\n–í–≤–µ–¥–∏—Ç–µ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ:", "–ø—É–Ω–∫—Ç")
        data['technical_proposal'] = input_list("\n–í–≤–µ–¥–∏—Ç–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ:", "–ø—É–Ω–∫—Ç")
        data['company_info'] = input_list("\n–í–≤–µ–¥–∏—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–º–ø–∞–Ω–∏–∏:", "–ø—É–Ω–∫—Ç")
        data['warranty_obligations'] = input_list("\n–í–≤–µ–¥–∏—Ç–µ –≥–∞—Ä–∞–Ω—Ç–∏–π–Ω—ã–µ –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞:", "–æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ")

    return data

def get_doc_full_name(doc_type):
    """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –µ–≥–æ —Ç–∏–ø—É"""
    doc_names = {
        "–¢–ó": "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ",
        "–ó–ù–ó": "–ó–∞–¥–∞–Ω–∏–µ –Ω–∞ –∑–∞–∫—É–ø–∫—É",
        "–ö–ü": "–ö–æ–Ω–∫—É—Ä—Å–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ"
    }
    return doc_names.get(doc_type, "–î–æ–∫—É–º–µ–Ω—Ç")

def main_menu():
    """–ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    assistant = AIAssistant()