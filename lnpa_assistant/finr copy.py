import os
import json
import pickle
import numpy as np
import re
from datetime import datetime
from typing import List, Dict, Tuple, Optional
import warnings
warnings.filterwarnings("ignore")

# –ë–∞–∑–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.feature_extraction.text import TfidfVectorizer
except ImportError as e:
    print(f"‚ùå –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏: {e}")
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sentence-transformers scikit-learn")
    exit(1)

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞
try:
    import torch
    from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("‚ö†Ô∏è Transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")

class DocumentProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    @staticmethod
    def extract_qa_pairs(text: str) -> List[Dict[str, str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        qa_pairs = []
        
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –±–ª–æ–∫–∏ –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º
        question_blocks = re.split(r'\n(?=–í–æ–ø—Ä–æ—Å:)', text.strip())
        
        for block in question_blocks:
            block = block.strip()
            if not block or '–í–æ–ø—Ä–æ—Å:' not in block:
                continue
                
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–æ–ø—Ä–æ—Å
            question_match = re.search(r'–í–æ–ø—Ä–æ—Å:\s*(.*?)(?=\n–û—Ç–≤–µ—Ç:|$)', block, re.DOTALL)
            if not question_match:
                continue
            question = question_match.group(1).strip()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç
            answer_match = re.search(r'–û—Ç–≤–µ—Ç:\s*(.*?)(?=\n–î–æ–∫—É–º–µ–Ω—Ç:|$)', block, re.DOTALL)
            if not answer_match:
                continue
            answer = answer_match.group(1).strip()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            document_match = re.search(r'–î–æ–∫—É–º–µ–Ω—Ç:\s*(.*?)(?=\n–°—Å—ã–ª–∫–∞:|$)', block, re.DOTALL)
            document = document_match.group(1).strip() if document_match else ""
            
            link_match = re.search(r'–°—Å—ã–ª–∫–∞:\s*(.*?)$', block, re.MULTILINE)
            link = link_match.group(1).strip() if link_match else ""
            
            if question and answer:
                qa_pairs.append({
                    'question': question,
                    'answer': answer,
                    'document': document,
                    'link': link,
                    'full_text': block
                })
        
        return qa_pairs
    
    @staticmethod
    def clean_text(text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n+', '\n', text)
        return text.strip()
    
    @staticmethod
    def extract_document_metadata(text: str) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        metadata = {}
        
        # –ü–æ–∏—Å–∫ –Ω–æ–º–µ—Ä–∞ –∑–∞–∫–æ–Ω–∞
        law_number = re.search(r'‚Ññ\s*(\d+-\w+)', text)
        if law_number:
            metadata['law_number'] = law_number.group(1)
        
        # –ü–æ–∏—Å–∫ –¥–∞—Ç—ã
        date_match = re.search(r'–æ—Ç\s*(\d{1,2}\s+\w+\s+\d{4})', text)
        if date_match:
            metadata['date'] = date_match.group(1)
        
        # –ü–æ–∏—Å–∫ –æ—Ä–≥–∞–Ω–∞
        if '–†–µ—Å–ø—É–±–ª–∏–∫–∏ –ë–µ–ª–∞—Ä—É—Å—å' in text:
            metadata['authority'] = '–†–µ—Å–ø—É–±–ª–∏–∫–∞ –ë–µ–ª–∞—Ä—É—Å—å'
        
        return metadata

class KnowledgeBase:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.embedder = SentenceTransformer(model_name)
        self.documents: List[Dict] = []
        self.embeddings: Optional[np.ndarray] = None
        self.tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words=None)
        self.tfidf_matrix = None
        self.index_file = "knowledge_base_index.pkl"
        
        print(f"‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å –º–æ–¥–µ–ª—å—é: {model_name}")
    
    def add_document_from_file(self, file_path: str) -> int:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except UnicodeDecodeError:
            with open(file_path, 'r', encoding='windows-1251') as f:
                content = f.read()
        
        qa_pairs = DocumentProcessor.extract_qa_pairs(content)
        metadata = DocumentProcessor.extract_document_metadata(content)
        
        added_count = 0
        for qa in qa_pairs:
            qa.update(metadata)
            qa['source_file'] = os.path.basename(file_path)
            qa['added_date'] = datetime.now().isoformat()
            self.documents.append(qa)
            added_count += 1
        
        print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω —Ñ–∞–π–ª {file_path}: –¥–æ–±–∞–≤–ª–µ–Ω–æ {added_count} –∑–∞–ø–∏—Å–µ–π")
        return added_count
    
    def load_documents_from_folder(self, folder_path: str) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –ø–∞–ø–∫–∏"""
        if not os.path.exists(folder_path):
            raise FileNotFoundError(f"–ü–∞–ø–∫–∞ {folder_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        
        txt_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]
        if not txt_files:
            raise ValueError(f"–í –ø–∞–ø–∫–µ {folder_path} –Ω–µ—Ç .txt —Ñ–∞–π–ª–æ–≤")
        
        total_added = 0
        for filename in txt_files:
            file_path = os.path.join(folder_path, filename)
            added = self.add_document_from_file(file_path)
            total_added += added
        
        print(f"üìö –í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {total_added} –∑–∞–ø–∏—Å–µ–π –∏–∑ {len(txt_files)} —Ñ–∞–π–ª–æ–≤")
        self.build_index()
    
    def build_index(self) -> None:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        if not self.documents:
            raise ValueError("–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
        
        print("üî® –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞...")
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        texts = []
        for doc in self.documents:
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –≤–æ–ø—Ä–æ—Å –∏ –æ—Ç–≤–µ—Ç –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
            combined_text = f"{doc['question']} {doc['answer']}"
            texts.append(combined_text)
        
        # –°—Ç—Ä–æ–∏–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
        self.embeddings = self.embedder.encode(texts, convert_to_numpy=True)
        
        # –°—Ç—Ä–æ–∏–º TF-IDF –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)
        
        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω –¥–ª—è {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    def save_index(self, filepath: str = None) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞"""
        filepath = filepath or self.index_file
        data = {
            'documents': self.documents,
            'embeddings': self.embeddings,
            'tfidf_vectorizer': self.tfidf_vectorizer,
            'tfidf_matrix': self.tfidf_matrix
        }
        with open(filepath, 'wb') as f:
            pickle.dump(data, f)
        print(f"üíæ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {filepath}")
    
    def load_index(self, filepath: str = None) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞"""
        filepath = filepath or self.index_file
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"–§–∞–π–ª –∏–Ω–¥–µ–∫—Å–∞ {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
        
        self.documents = data['documents']
        self.embeddings = data['embeddings']
        self.tfidf_vectorizer = data['tfidf_vectorizer']
        self.tfidf_matrix = data['tfidf_matrix']
        
        print(f"üì• –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {filepath}: {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    def search(self, query: str, top_k: int = 5, method: str = "combined") -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        if not self.documents or self.embeddings is None:
            raise ValueError("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞ –∏–ª–∏ –Ω–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∞")
        
        query_cleaned = DocumentProcessor.clean_text(query)
        
        if method == "semantic":
            return self._semantic_search(query_cleaned, top_k)
        elif method == "tfidf":
            return self._tfidf_search(query_cleaned, top_k)
        else:  # combined
            return self._combined_search(query_cleaned, top_k)
    
    def _semantic_search(self, query: str, top_k: int) -> List[Dict]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
        query_embedding = self.embedder.encode([query], convert_to_numpy=True)
        similarities = cosine_similarity(query_embedding, self.embeddings)[0]
        
        top_indices = similarities.argsort()[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            result = self.documents[idx].copy()
            result['similarity_score'] = float(similarities[idx])
            result['search_method'] = 'semantic'
            results.append(result)
        
        return results
    
    def _tfidf_search(self, query: str, top_k: int) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ TF-IDF"""
        query_tfidf = self.tfidf_vectorizer.transform([query])
        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)[0]
        
        top_indices = similarities.argsort()[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0:  # –¢–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                result = self.documents[idx].copy()
                result['similarity_score'] = float(similarities[idx])
                result['search_method'] = 'tfidf'
                results.append(result)
        
        return results
    
    def _combined_search(self, query: str, top_k: int) -> List[Dict]:
        """–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫"""
        semantic_results = self._semantic_search(query, top_k)
        tfidf_results = self._tfidf_search(query, top_k)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –≤–µ—Å–∞–º–∏
        combined_scores = {}
        
        for result in semantic_results:
            doc_id = id(result['full_text'])
            combined_scores[doc_id] = {
                'document': result,
                'semantic_score': result['similarity_score'],
                'tfidf_score': 0.0
            }
        
        for result in tfidf_results:
            doc_id = id(result['full_text'])
            if doc_id in combined_scores:
                combined_scores[doc_id]['tfidf_score'] = result['similarity_score']
            else:
                combined_scores[doc_id] = {
                    'document': result,
                    'semantic_score': 0.0,
                    'tfidf_score': result['similarity_score']
                }
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä
        for doc_id in combined_scores:
            semantic_weight = 0.7
            tfidf_weight = 0.3
            combined_scores[doc_id]['combined_score'] = (
                semantic_weight * combined_scores[doc_id]['semantic_score'] +
                tfidf_weight * combined_scores[doc_id]['tfidf_score']
            )
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —Å–∫–æ—Ä—É
        sorted_results = sorted(combined_scores.values(), 
                              key=lambda x: x['combined_score'], 
                              reverse=True)
        
        final_results = []
        for item in sorted_results[:top_k]:
            result = item['document'].copy()
            result['similarity_score'] = item['combined_score']
            result['search_method'] = 'combined'
            final_results.append(result)
        
        return final_results

class DocumentGenerator:
    """–ö–ª–∞—Å—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã)"""
    
    def __init__(self, knowledge_base: KnowledgeBase):
        self.kb = knowledge_base
        self.templates = self._load_templates()
    
    def _load_templates(self) -> Dict[str, str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —à–∞–±–ª–æ–Ω–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        return {
            'tz': '''–¢–ï–•–ù–ò–ß–ï–°–ö–û–ï –ó–ê–î–ê–ù–ò–ï
–Ω–∞ {subject}

1. –û–ë–©–ò–ï –ü–û–õ–û–ñ–ï–ù–ò–Ø
{general_provisions}

2. –¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –í–´–ü–û–õ–ù–ï–ù–ò–Æ –†–ê–ë–û–¢
{requirements}

3. –°–†–û–ö–ò –í–´–ü–û–õ–ù–ï–ù–ò–Ø
{timeline}

4. –û–¢–í–ï–¢–°–¢–í–ï–ù–ù–û–°–¢–¨ –°–¢–û–†–û–ù
{responsibility}
''',
            'znz': '''–ó–ê–î–ê–ù–ò–ï –ù–ê –ó–ê–ö–£–ü–ö–£

–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∑–∞–∫—É–ø–∫–∏: {procurement_name}
–ó–∞–∫–∞–∑—á–∏–∫: {customer}
–ü—Ä–µ–¥–º–µ—Ç –∑–∞–∫—É–ø–∫–∏: {subject}

–¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
{technical_requirements}

–ö–û–ú–ú–ï–†–ß–ï–°–ö–ò–ï –£–°–õ–û–í–ò–Ø:
{commercial_conditions}
''',
            'kp': '''–ö–û–ù–ö–£–†–°–ù–û–ï –ü–†–ï–î–õ–û–ñ–ï–ù–ò–ï

–£—á–∞—Å—Ç–Ω–∏–∫: {participant}
–ü—Ä–µ–¥–º–µ—Ç –∫–æ–Ω–∫—É—Ä—Å–∞: {subject}

–¢–ï–•–ù–ò–ß–ï–°–ö–û–ï –ü–†–ï–î–õ–û–ñ–ï–ù–ò–ï:
{technical_proposal}

–ö–û–ú–ú–ï–†–ß–ï–°–ö–û–ï –ü–†–ï–î–õ–û–ñ–ï–ù–ò–ï:
{commercial_proposal}
'''
        }
    
    def generate_document(self, doc_type: str, context: Dict[str, str]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ —à–∞–±–ª–æ–Ω—É"""
        if doc_type not in self.templates:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞: {doc_type}")
        
        template = self.templates[doc_type]
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º —à–∞–±–ª–æ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        try:
            return template.format(**context)
        except KeyError as e:
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä: {e}")

class AIAssistant:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
    
    def __init__(self):
        self.kb = KnowledgeBase()
        self.generator = DocumentGenerator(self.kb)
        self.llm_pipeline = None
        
        if TRANSFORMERS_AVAILABLE:
            self._init_llm()
    
    def _init_llm(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            model_name = "sberbank-ai/rugpt3small_based_on_gpt2"
            self.llm_pipeline = pipeline(
                "text-generation", 
                model=model_name,
                tokenizer=model_name,
                device=0 if torch.cuda.is_available() else -1
            )
            print("ü§ñ –Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å: {e}")
    
    def setup_knowledge_base(self, documents_folder: str) -> None:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        try:
            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å
            self.kb.load_index()
            print("üì• –ó–∞–≥—Ä—É–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å")
        except FileNotFoundError:
            # –°—Ç—Ä–æ–∏–º –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
            print("üî® –°—Ç—Ä–æ–∏–º –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å...")
            self.kb.load_documents_from_folder(documents_folder)
            self.kb.save_index()
    
    def ask(self, question: str, use_llm: bool = False) -> Dict[str, any]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤"""
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        results = self.kb.search(question, top_k=3)
        
        if not results:
            return {
                'answer': "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –≤–∞—à–µ–º—É –≤–æ–ø—Ä–æ—Å—É.",
                'sources': [],
                'confidence': 0.0
            }
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        best_match = results[0]
        
        if use_llm and self.llm_pipeline:
            answer = self._generate_llm_answer(question, results)
        else:
            answer = best_match['answer']
        
        return {
            'answer': answer,
            'sources': [
                {
                    'document': r['document'],
                    'link': r['link'],
                    'confidence': r['similarity_score']
                } for r in results
            ],
            'confidence': best_match['similarity_score']
        }
    
    def _generate_llm_answer(self, question: str, context_results: List[Dict]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM"""
        context = "\n".join([
            f"–í–æ–ø—Ä–æ—Å: {r['question']}\n–û—Ç–≤–µ—Ç: {r['answer']}"
            for r in context_results[:2]
        ])
        
        prompt = f"""–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–µ –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ë–µ–ª–∞—Ä—É—Å—å. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –¥–∞–≤–∞—Ç—å —Ç–æ—á–Ω—ã–µ, –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∞–≤–æ–≤—ã—Ö –∞–∫—Ç–æ–≤, –¥–µ–π—Å—Ç–≤—É—é—â–∏—Ö –Ω–∞ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ –†–ë. –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –°—Å—ã–ª–∞–π—Å—è –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∏–∑–±–µ–≥–∞–π –¥–æ–º—ã—Å–ª–æ–≤, —Å–æ–±–ª—é–¥–∞–π —é—Ä–∏–¥–∏—á–µ—Å–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å.


–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {question}
–û—Ç–≤–µ—Ç:"""
        
        try:
            response = self.llm_pipeline(
                prompt,
                max_length=len(prompt.split()) + 100,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True
            )
            
            generated_text = response[0]["generated_text"]
            answer = generated_text.split("–û—Ç–≤–µ—Ç:")[-1].strip()
            return answer
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM: {e}")
            return context_results[0]['answer']
    
    def get_statistics(self) -> Dict[str, any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        return {
            'total_documents': len(self.kb.documents),
            'unique_sources': len(set(doc['source_file'] for doc in self.kb.documents)),
            'has_embeddings': self.kb.embeddings is not None,
            'has_llm': self.llm_pipeline is not None
        }

def main_menu():
    """–ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    assistant = AIAssistant()
    
    while True:
        print("\n" + "="*60)
        print("ü§ñ AI-–ê–°–°–ò–°–¢–ï–ù–¢ –î–õ–Ø –†–ê–ë–û–¢–´ –° –õ–ü–ê/–õ–ù–ü–ê")
        print("="*60)
        print("1. üìö –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π")
        print("2. ‚ùì –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å")
        print("3. üîç –ü–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ")
        print("4. üìÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞")
        print("5. üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
        print("6. üíæ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–º")
        print("7. ‚ùå –í—ã—Ö–æ–¥")
        
        choice = input("\n–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ (1-7): ").strip()
        
        if choice == "1":
            setup_kb_menu(assistant)
        elif choice == "2":
            ask_question_menu(assistant)
        elif choice == "3":
            search_menu(assistant)
        elif choice == "4":
            generate_document_menu(assistant)
        elif choice == "5":
            show_statistics(assistant)
        elif choice == "6":
            manage_index_menu(assistant)
        elif choice == "7":
            print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        else:
            print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä!")

def setup_kb_menu(assistant: AIAssistant):
    """–ú–µ–Ω—é –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
    print("\nüìö –ù–ê–°–¢–†–û–ô–ö–ê –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô")
    folder = input("–í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ [./document]: ").strip() or "./document"
    
    try:
        assistant.setup_knowledge_base(folder)
        print("‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏: {e}")

def ask_question_menu(assistant: AIAssistant):
    """–ú–µ–Ω—é –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤"""
    if not assistant.kb.documents:
        print("‚ùå –°–Ω–∞—á–∞–ª–∞ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π!")
        return
    
    print("\n‚ùì –†–ï–ñ–ò–ú –í–û–ü–†–û–°–û–í")
    print("–í–≤–µ–¥–∏—Ç–µ '–Ω–∞–∑–∞–¥' –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞ –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é")
    
    while True:
        question = input("\nüë§ –í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()
        
        if question.lower() in ['–Ω–∞–∑–∞–¥', 'back', '–≤—ã—Ö–æ–¥']:
            break
        
        if not question:
            continue
        
        use_llm = input("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞? (y/n) [n]: ").strip().lower() == 'y'
        
        try:
            result = assistant.ask(question, use_llm=use_llm)
            
            print(f"\nü§ñ –û—Ç–≤–µ—Ç (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.2%}):")
            print(result['answer'])
            
            if result['sources']:
                print(f"\nüìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏ ({len(result['sources'])}):")
                for i, source in enumerate(result['sources'], 1):
                    print(f"{i}. {source['document']} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {source['confidence']:.2%})")
                    if source['link']:
                        print(f"   üîó {source['link']}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

def search_menu(assistant: AIAssistant):
    """–ú–µ–Ω—é –ø–æ–∏—Å–∫–∞"""
    if not assistant.kb.documents:
        print("‚ùå –°–Ω–∞—á–∞–ª–∞ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π!")
        return
    
    print("\nüîç –†–ï–ñ–ò–ú –ü–û–ò–°–ö–ê")
    query = input("–í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å: ").strip()
    
    if not query:
        return
    
    method = input("–ú–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ (semantic/tfidf/combined) [combined]: ").strip() or "combined"
    top_k = input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ [5]: ").strip() or "5"
    
    try:
        top_k = int(top_k)
        results = assistant.kb.search(query, top_k=top_k, method=method)
        
        print(f"\nüìã –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
        for i, result in enumerate(results, 1):
            print(f"\n{i}. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {result['similarity_score']:.2%}")
            print(f"‚ùì –í–æ–ø—Ä–æ—Å: {result['question'][:100]}...")
            print(f"üí¨ –û—Ç–≤–µ—Ç: {result['answer'][:200]}...")
            if result['document']:
                print(f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç: {result['document']}")
            
    except ValueError:
        print("‚ùå –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —á–∏—Å–ª–æ–º!")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")

def generate_document_menu(assistant: AIAssistant):
    """–ú–µ–Ω—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    print("\nüìÑ –ì–ï–ù–ï–†–ê–¶–ò–Ø –î–û–ö–£–ú–ï–ù–¢–û–í")
    print("–î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–∏–ø—ã:")
    print("1. tz - –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ")
    print("2. znz - –ó–∞–¥–∞–Ω–∏–µ –Ω–∞ –∑–∞–∫—É–ø–∫—É")
    print("3. kp - –ö–æ–Ω–∫—É—Ä—Å–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ")
    
    doc_type = input("–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ (tz/znz/kp): ").strip().lower()
    
    if doc_type not in ['tz', 'znz', 'kp']:
        print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞!")
        return
    
    # –°–æ–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    context = {}
    
    if doc_type == 'tz':
        context['subject'] = input("–ü—Ä–µ–¥–º–µ—Ç –¢–ó: ").strip()
        context['general_provisions'] = input("–û–±—â–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è: ").strip()
        context['requirements'] = input("–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: ").strip()
        context['timeline'] = input("–°—Ä–æ–∫–∏: ").strip()
        context['responsibility'] = input("–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å: ").strip()
    
    elif doc_type == 'znz':
        context['procurement_name'] = input("–ù–∞–∑–≤–∞–Ω–∏–µ –∑–∞–∫—É–ø–∫–∏: ").strip()
        context['customer'] = input("–ó–∞–∫–∞–∑—á–∏–∫: ").strip()
        context['subject'] = input("–ü—Ä–µ–¥–º–µ—Ç –∑–∞–∫—É–ø–∫–∏: ").strip()
        context['technical_requirements'] = input("–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è: ").strip()
        context['commercial_conditions'] = input("–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ —É—Å–ª–æ–≤–∏—è: ").strip()
    
    elif doc_type == 'kp':
        context['participant'] = input("–£—á–∞—Å—Ç–Ω–∏–∫: ").strip()
        context['subject'] = input("–ü—Ä–µ–¥–º–µ—Ç –∫–æ–Ω–∫—É—Ä—Å–∞: ").strip()
        context['technical_proposal'] = input("–¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: ").strip()
        context['commercial_proposal'] = input("–ö–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: ").strip()
    
    try:
        document = assistant.generator.generate_document(doc_type, context)
        print(f"\nüìÑ –°–ì–ï–ù–ï–†–ò–†–û–í–ê–ù–ù–´–ô –î–û–ö–£–ú–ï–ù–¢:")
        print("="*60)
        print(document)
        print("="*60)
        
        save = input("\n–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –≤ —Ñ–∞–π–ª? (y/n): ").strip().lower()
        if save == 'y':
            filename = input("–ò–º—è —Ñ–∞–π–ª–∞ (–±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è): ").strip()
            if filename:
                with open(f"{filename}.txt", 'w', encoding='utf-8') as f:
                    f.write(document)
                print(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {filename}.txt")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")

def show_statistics(assistant: AIAssistant):
    """–ü–æ–∫–∞–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
    stats = assistant.get_statistics()
    
    print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ò–°–¢–ï–ú–´")
    print(f"üìö –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {stats['total_documents']}")
    print(f"üìÅ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {stats['unique_sources']}")
    print(f"üîç –ò–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω: {'‚úÖ' if stats['has_embeddings'] else '‚ùå'}")
    print(f"ü§ñ LLM –¥–æ—Å—Ç—É–ø–µ–Ω: {'‚úÖ' if stats['has_llm'] else '‚ùå'}")
    
    if assistant.kb.documents:
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        sources = {}
        for doc in assistant.kb.documents:
            source = doc['source_file']
            sources[source] = sources.get(source, 0) + 1
        
        print(f"\nüìÅ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ñ–∞–π–ª–∞–º:")
        for source, count in sources.items():
            print(f"  {source}: {count} –∑–∞–ø–∏—Å–µ–π")

def manage_index_menu(assistant: AIAssistant):
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–º"""
    print("\nüíæ –£–ü–†–ê–í–õ–ï–ù–ò–ï –ò–ù–î–ï–ö–°–û–ú")
    print("1. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω–¥–µ–∫—Å")
    print("2. –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω–¥–µ–∫—Å")
    print("3. –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å")
    
    choice = input("–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ (1-3): ").strip()
    
    if choice == "1":
        try:
            assistant.kb.save_index()
            print("‚úÖ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
    
    elif choice == "2":
        try:
            assistant.kb.load_index()
            print("‚úÖ –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
    
    elif choice == "3":
        if assistant.kb.documents:
            try:
                assistant.kb.build_index()
                print("‚úÖ –ò–Ω–¥–µ–∫—Å –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è: {e}")
        else:
            print("‚ùå –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")

if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –õ–ü–ê/–õ–ù–ü–ê...")
    print("üìã –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π...")
    
    try:
        main_menu()
    except KeyboardInterrupt:
        print("\n\nüëã –†–∞–±–æ—Ç–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()